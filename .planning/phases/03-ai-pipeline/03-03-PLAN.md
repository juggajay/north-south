---
phase: 03-ai-pipeline
plan: 03
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - src/lib/ai/depth-estimation.ts
  - src/lib/ai/render-generation.ts
  - convex/ai.ts
autonomous: true
user_setup:
  - service: google-ai
    why: "Gemini API for render generation (Nano Banana Pro)"
    env_vars:
      - name: GEMINI_API_KEY
        source: "Google AI Studio -> Get API Key"
    dashboard_config:
      - task: "Enable Gemini API access"
        location: "Google AI Studio (aistudio.google.com)"

must_haves:
  truths:
    - "Dimensions can be estimated from space analysis"
    - "Confidence tier is clearly communicated (Basic tier for MVP)"
    - "3 styled renders can be generated from a photo"
    - "Renders preserve the original space context"
  artifacts:
    - path: "src/lib/ai/depth-estimation.ts"
      provides: "Dimension estimation from Claude analysis"
      exports: ["estimateDimensions", "getDimensionTierLabel"]
    - path: "src/lib/ai/render-generation.ts"
      provides: "Nano Banana Pro render client"
      exports: ["generateRenders", "getStylesForSpace"]
  key_links:
    - from: "src/lib/ai/depth-estimation.ts"
      to: "SpaceAnalysis"
      via: "type import"
      pattern: "import.*SpaceAnalysis"
    - from: "convex/ai.ts"
      to: "@google/genai"
      via: "SDK import"
      pattern: "GoogleGenerativeAI"
---

<objective>
Implement dimension estimation from Claude Vision analysis and render generation via Gemini/Nano Banana Pro.

Purpose: Complete the AI backend - extract dimensions with confidence tiers, generate 3 styled renders showing joinery in user's space.
Output: Two AI clients that transform analysis into dimensions and renders for the carousel.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-ai-pipeline/03-RESEARCH.md
@.planning/phases/03-ai-pipeline/03-01-SUMMARY.md
@src/types/ai-pipeline.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Dimension Estimation Module</name>
  <files>src/lib/ai/depth-estimation.ts</files>
  <action>
Create dimension estimation that processes Claude Vision analysis.

For MVP, use Claude's dimension estimates directly (Basic tier, +/-15% accuracy). This avoids the complexity of Depth Anything V2 integration while still providing useful estimates.

```typescript
import { SpaceAnalysis, Dimensions, DimensionConfidence } from '@/types/ai-pipeline';

/**
 * Get human-readable label for dimension confidence tier
 * Per CONTEXT.md: "High confidence" vs "Verify dimensions"
 */
export function getDimensionTierLabel(confidence: DimensionConfidence): string {
  switch (confidence) {
    case 'precision':
      return 'High confidence';
    case 'enhanced':
      return 'High confidence';
    case 'standard':
      return 'Verify dimensions';
    case 'basic':
    default:
      return 'Verify dimensions';
  }
}

/**
 * Calculate confidence percentage from tier
 */
function getConfidencePercent(tier: DimensionConfidence): number {
  switch (tier) {
    case 'precision': return 98;  // +/-2% (LiDAR)
    case 'enhanced': return 95;   // +/-5% (multi-photo)
    case 'standard': return 90;   // +/-10% (photo + reference)
    case 'basic':
    default: return 85;           // +/-15% (single photo)
  }
}

/**
 * Apply reasonable constraints to dimension estimates
 * Catches obvious errors in AI estimation
 */
function constrainDimensions(raw: {
  width: number;
  depth: number;
  height: number;
}): { width: number; depth: number; height: number } {
  return {
    // Min 500mm, max 6000mm for cabinet spaces
    width: Math.max(500, Math.min(6000, raw.width)),
    depth: Math.max(300, Math.min(2000, raw.depth)),
    // Australian ceiling heights typically 2400-3000mm
    height: Math.max(2100, Math.min(3500, raw.height)),
  };
}

/**
 * Estimate dimensions from Claude Vision space analysis
 * MVP uses Basic tier (single photo, +/-15% accuracy)
 *
 * Future tiers:
 * - Standard: Add reference object detection (credit card)
 * - Enhanced: Multiple photos combined
 * - Precision: LiDAR data (Phase 04+)
 */
export function estimateDimensions(
  analysis: SpaceAnalysis,
  tier: DimensionConfidence = 'basic'
): Dimensions {
  // Use Claude's estimates, applying sanity constraints
  const constrained = constrainDimensions({
    width: analysis.estimatedWidth,
    depth: analysis.estimatedDepth,
    height: analysis.estimatedHeight,
  });

  const confidencePercent = getConfidencePercent(tier);
  const tierLabel = getDimensionTierLabel(tier);

  return {
    width: constrained.width,
    depth: constrained.depth,
    height: constrained.height,
    confidence: tier,
    confidencePercent,
    tierLabel,
  };
}

/**
 * Format dimensions for display
 * Returns string like "2400 x 600 x 2400mm"
 */
export function formatDimensions(dims: Dimensions): string {
  return `${dims.width} x ${dims.depth} x ${dims.height}mm`;
}
```

This approach:
- Uses Claude Vision's built-in dimension estimation
- Applies reasonable min/max constraints
- Clearly communicates confidence tier
- Leaves room for future Depth Anything V2 integration
  </action>
  <verify>`npm run typecheck` passes, functions exported</verify>
  <done>Dimension estimation with confidence tiers ready</done>
</task>

<task type="auto">
  <name>Task 2: Create Render Generation Client</name>
  <files>src/lib/ai/render-generation.ts</files>
  <action>
Create client-side helpers for render generation:

```typescript
import { SpaceAnalysis, StyleMatch, Render, Dimensions } from '@/types/ai-pipeline';

/**
 * Polytec finish styles that work for joinery
 * Maps aesthetic to appropriate Polytec color codes
 */
const POLYTEC_STYLES: Record<string, StyleMatch[]> = {
  modern: [
    { id: 'modern-white', name: 'Modern White', polytech: ['Polar White', 'Classic White'] },
    { id: 'modern-grey', name: 'Concrete Grey', polytech: ['Shannon Oak', 'Stone Grey'] },
    { id: 'modern-charcoal', name: 'Modern Charcoal', polytech: ['Char Oak', 'Black Wenge'] },
  ],
  traditional: [
    { id: 'trad-oak', name: 'Classic Oak', polytech: ['Natural Oak', 'Honey Elm'] },
    { id: 'trad-walnut', name: 'Warm Walnut', polytech: ['Walnut Nuance', 'Opera Walnut'] },
    { id: 'trad-cream', name: 'Cream Shaker', polytech: ['Vanilla Shake', 'Classic White'] },
  ],
  industrial: [
    { id: 'ind-dark', name: 'Industrial Dark', polytech: ['Black Wenge', 'Char Oak'] },
    { id: 'ind-concrete', name: 'Raw Concrete', polytech: ['Stone Grey', 'Concrete'] },
    { id: 'ind-metal', name: 'Brushed Metal', polytech: ['Aluminium', 'Titanium'] },
  ],
  coastal: [
    { id: 'coast-white', name: 'Coastal White', polytech: ['Polar White', 'Classic White'] },
    { id: 'coast-timber', name: 'Beach Timber', polytech: ['Bleached Walnut', 'Natural Oak'] },
    { id: 'coast-blue', name: 'Ocean Blue', polytech: ['Navy Blue', 'Riviera'] },
  ],
  scandinavian: [
    { id: 'scandi-light', name: 'Nordic Light', polytech: ['Polar White', 'Birch Ply'] },
    { id: 'scandi-oak', name: 'Scandi Oak', polytech: ['Natural Oak', 'Prime Oak'] },
    { id: 'scandi-grey', name: 'Stockholm Grey', polytech: ['Shannon Oak', 'Pale Grey'] },
  ],
};

/**
 * Get appropriate styles for a space based on its aesthetic
 * Returns 3 style options matching the detected style
 */
export function getStylesForSpace(analysis: SpaceAnalysis): StyleMatch[] {
  const aesthetic = analysis.styleAesthetic;
  const styles = POLYTEC_STYLES[aesthetic] || POLYTEC_STYLES['modern'];
  return styles.slice(0, 3);
}

/**
 * Build render prompt for Gemini image generation
 */
export function buildRenderPrompt(
  style: StyleMatch,
  analysis: SpaceAnalysis,
  dimensions: Dimensions
): string {
  return `Generate a photorealistic interior render showing custom joinery/cabinetry in this ${analysis.roomType}.

Style: ${style.name}
Materials: ${style.polytech.join(', ')} (Polytec finishes)
Space dimensions: ${dimensions.width}mm wide x ${dimensions.depth}mm deep x ${dimensions.height}mm high

Requirements:
- Keep the original room context (walls, floor, ceiling, windows visible)
- Add modern ${style.name.toLowerCase()} style cabinetry/joinery that fits the space
- Use ${style.polytech[0]} as the primary material finish
- Realistic lighting matching ${analysis.lightingConditions} conditions
- Maintain the ${analysis.styleAesthetic} aesthetic of the space
- Show professional custom joinery, not flat-pack furniture

Output a single photorealistic image showing the transformed space.`;
}

/**
 * Parse render result from Gemini API response
 */
export function parseRenderResult(
  response: any,
  style: StyleMatch,
  index: number
): Render {
  // Handle different response formats from Gemini
  let imageData: string;

  if (response.candidates?.[0]?.content?.parts?.[0]?.inlineData?.data) {
    imageData = response.candidates[0].content.parts[0].inlineData.data;
  } else if (response.image) {
    imageData = response.image;
  } else {
    throw new Error('No image data in Gemini response');
  }

  return {
    id: `render-${style.id}-${index}`,
    styleLabel: style.name,
    styleId: style.id,
    imageUrl: `data:image/jpeg;base64,${imageData}`,
    imageBase64: imageData,
  };
}
```

Note: Actual Gemini API calls happen in the Convex action (next task) to keep API key secure.
  </action>
  <verify>`npm run typecheck` passes, functions exported</verify>
  <done>Client-side render generation helpers ready</done>
</task>

<task type="auto">
  <name>Task 3: Add Render Generation Convex Action</name>
  <files>convex/ai.ts</files>
  <action>
Add render generation action to the existing convex/ai.ts file:

```typescript
// Add to existing imports
import { GoogleGenerativeAI } from '@google/genai';

// Add this action to the file:

export const generateRendersAction = action({
  args: {
    imageBase64: v.string(),
    styles: v.array(v.object({
      id: v.string(),
      name: v.string(),
      polytech: v.array(v.string()),
    })),
    roomType: v.string(),
    dimensions: v.object({
      width: v.number(),
      depth: v.number(),
      height: v.number(),
    }),
    styleAesthetic: v.string(),
    lightingConditions: v.string(),
  },
  handler: async (ctx, args) => {
    const apiKey = process.env.GEMINI_API_KEY;
    if (!apiKey) {
      throw new Error('GEMINI_API_KEY not configured');
    }

    const genAI = new GoogleGenerativeAI(apiKey);

    // Try Nano Banana Pro (Gemini 3 Pro Image), fall back to Flash
    let modelName = 'gemini-2.0-flash-exp';
    try {
      // Test if newer model is available
      const testModel = genAI.getGenerativeModel({ model: 'gemini-3-pro-image' });
      modelName = 'gemini-3-pro-image';
    } catch {
      // Fall back to flash
    }

    const model = genAI.getGenerativeModel({ model: modelName });

    const renders = await Promise.all(
      args.styles.slice(0, 3).map(async (style, index) => {
        const prompt = `Generate a photorealistic interior render showing custom joinery/cabinetry in this ${args.roomType}.

Style: ${style.name}
Materials: ${style.polytech.join(', ')} (Polytec finishes)
Space dimensions: ${args.dimensions.width}mm wide x ${args.dimensions.depth}mm deep x ${args.dimensions.height}mm high

Requirements:
- Keep the original room context (walls, floor, ceiling visible)
- Add modern ${style.name.toLowerCase()} style cabinetry
- Use ${style.polytech[0]} as primary material
- Realistic ${args.lightingConditions} lighting
- ${args.styleAesthetic} aesthetic
- Professional custom joinery quality

Output a single photorealistic image.`;

        try {
          const result = await model.generateContent({
            contents: [{
              role: 'user',
              parts: [
                { text: prompt },
                { inlineData: { mimeType: 'image/jpeg', data: args.imageBase64 } },
              ],
            }],
          });

          const response = await result.response;

          // Try to extract image from response
          const imagePart = response.candidates?.[0]?.content?.parts?.find(
            (part: any) => part.inlineData
          );

          if (imagePart?.inlineData?.data) {
            return {
              success: true,
              render: {
                id: `render-${style.id}-${index}`,
                styleLabel: style.name,
                styleId: style.id,
                imageBase64: imagePart.inlineData.data,
              },
            };
          }

          // If no image, might be text response describing what would be rendered
          // Return placeholder indicating render couldn't be generated
          return {
            success: false,
            error: 'Model returned text instead of image',
            styleId: style.id,
          };
        } catch (error) {
          console.error(`Render generation failed for ${style.name}:`, error);
          return {
            success: false,
            error: error instanceof Error ? error.message : 'Unknown error',
            styleId: style.id,
          };
        }
      })
    );

    // Return all renders, including any failures
    const successful = renders.filter((r) => r.success);
    const failed = renders.filter((r) => !r.success);

    return {
      renders: successful.map((r) => r.render),
      failedCount: failed.length,
      errors: failed.map((f) => ({ styleId: f.styleId, error: f.error })),
    };
  },
});
```

Make sure GEMINI_API_KEY is set in Convex environment variables.

Note: If image generation isn't available in the model, the action gracefully handles this and returns appropriate errors. The UI can show a placeholder or retry message.
  </action>
  <verify>`npm run typecheck` passes, action exported</verify>
  <done>Render generation Convex action with fallback handling</done>
</task>

</tasks>

<verification>
- [ ] `npm run typecheck` passes
- [ ] src/lib/ai/depth-estimation.ts exports estimateDimensions, getDimensionTierLabel, formatDimensions
- [ ] src/lib/ai/render-generation.ts exports getStylesForSpace, buildRenderPrompt, parseRenderResult
- [ ] convex/ai.ts exports generateRendersAction
- [ ] Style mapping covers all 5 aesthetics with 3 options each
</verification>

<success_criteria>
- Dimension estimation applies sanity constraints
- Confidence tiers clearly labeled ("Verify dimensions" for Basic)
- Style-to-Polytec mapping covers all aesthetics
- Render generation handles model fallback gracefully
- Partial failures don't break the whole pipeline
</success_criteria>

<output>
After completion, create `.planning/phases/03-ai-pipeline/03-03-SUMMARY.md`
</output>
